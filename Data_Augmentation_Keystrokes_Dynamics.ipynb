{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Augmentation - Keystrokes Dynamics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Ka30FJh0QAnzPokn8tqtF0cfJb9a8Qbt",
      "authorship_tag": "ABX9TyPk9euPJvXShavVi6sd4604",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheYoungBeast/Data-Augmentation-Keystrokes-Dynamics/blob/main/Data_Augmentation_Keystrokes_Dynamics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iS9qIUysQ1jR",
        "outputId": "736c5344-16e5-4c0d-d700-de44bab65973"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from numpy.ma.core import array"
      ],
      "metadata": {
        "id": "XFbsJeKD6jVX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Data Types</h1>\n"
      ],
      "metadata": {
        "id": "RKFU25dzbQOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import NamedTuple\n",
        "\n",
        "class Set(NamedTuple):\n",
        "  id: int\n",
        "  keystrokes: array = []\n",
        "  aug: bool = False\n",
        "\n",
        "class Keystroke(NamedTuple):\n",
        "  key: str\n",
        "  up: int\n",
        "  down: int\n",
        "  dtime: int"
      ],
      "metadata": {
        "id": "E4XG8mMmYVmA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Loading dataset</h1>"
      ],
      "metadata": {
        "id": "yDaz99UKbVg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = {};\n",
        "\n",
        "for i in range(1, 15):\n",
        "  dataset[str(i)] = []\n",
        "\n",
        "  for j in range(0, 3):\n",
        "    filepath = \"drive/MyDrive/Keystrokes/#\" + str(i).zfill(2) + \"_\" + str(j)  +\".txt\"\n",
        "    dataset[str(i)].append(Set(i-1, []))\n",
        "\n",
        "    with open(filepath, 'r') as f:\n",
        "            line = f.readline()\n",
        "            while line:\n",
        "\n",
        "              if len(line.strip().split(',')) == 3:\n",
        "                key, down, up = line.strip().split(',')\n",
        "                keystroke = Keystroke(key, int(up), int(down), int(down) - int(up))\n",
        "                dataset[str(i)][-1].keystrokes.append(keystroke)\n",
        "\n",
        "              line = f.readline()"
      ],
      "metadata": {
        "id": "m-H1GaSiSkLY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#def convert_to_datasets(dataset):\n",
        "#  train_data = []\n",
        "#  test_data = []\n",
        "#  train_labels = []\n",
        "#  test_labels = []\n",
        "#\n",
        "#  for key in dataset:\n",
        "#    for i in range(0, len(dataset[key])-1):\n",
        "#      train_data.append([])\n",
        "#      for k in dataset[key][i].keystrokes:\n",
        "#        train_data[-1].append(k.dtime)\n",
        "#\n",
        "#      train_labels.append(dataset[key][i].id)\n",
        "#    \n",
        "#    test_data.append([])\n",
        "#    for k in dataset[key][-1].keystrokes:\n",
        "#      test_data[-1].append(k.dtime)\n",
        "#\n",
        "#    test_labels.append(dataset[key][-1].id)\n",
        "#\n",
        "#  return train_data, train_labels, test_data, test_labels"
      ],
      "metadata": {
        "id": "PBhRhE9wc5Me"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Preprocessing and Normalization</h1>"
      ],
      "metadata": {
        "id": "ha3vj-ZcbHm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(set):\n",
        "  dict = {}\n",
        "  count = {}\n",
        "\n",
        "  for keystroke in set.keystrokes:\n",
        "    if not keystroke.key:\n",
        "      continue\n",
        "\n",
        "    if keystroke.key not in dict:\n",
        "      dict[keystroke.key] = 0\n",
        "      count[keystroke.key] = 0\n",
        "\n",
        "    dict[keystroke.key] += abs(keystroke.dtime) # Absolute value, dwell time\n",
        "    count[keystroke.key] += 1\n",
        "\n",
        "  # Average dwell time per Key\n",
        "  avgs = []\n",
        "  \n",
        "  for key in dict:\n",
        "    avgs.append(math.floor(dict[key]/count[key]))\n",
        "\n",
        "  if len(avgs) is not 27:\n",
        "    raise Exception('Dataset is corrupted', 'The number of unique keys in the set exceeds 27')\n",
        "\n",
        "  return avgs, set.id"
      ],
      "metadata": {
        "id": "4RpNBKn55OgO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(data):\n",
        "  norm_data = []\n",
        "  max = np.max(data)\n",
        "\n",
        "  for i in range(len(data)):\n",
        "    norm_data.append(data[i] / max) # local normalization\n",
        "\n",
        "  return norm_data"
      ],
      "metadata": {
        "id": "8_vjuHxE-PFD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_all(dataset):\n",
        "  train_data = []\n",
        "  test_data = []\n",
        "  train_labels = []\n",
        "  test_labels = []\n",
        "\n",
        "  for key in dataset:\n",
        "    for i in range(0, len(dataset[key])-1): # skip last set\n",
        "      tdata, sid = preprocess(dataset[key][i])\n",
        "\n",
        "      train_data.append(normalize(tdata)) # normalization\n",
        "      train_labels.append(sid)\n",
        "    \n",
        "    tdata, sid = preprocess(dataset[key][-1]) # treat last set as test data\n",
        "    test_data.append(normalize(tdata)) # normalizaton\n",
        "    test_labels.append(sid)\n",
        "\n",
        "  return train_data, train_labels, test_data, test_labels"
      ],
      "metadata": {
        "id": "r7EOKCwH-RxL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, train_labels, test_data, test_labels = convert_all(dataset)\n",
        "\n",
        "assert len(test_data) == len(test_labels), 'The size of data is not equal'\n",
        "assert len(train_data) == len(train_labels), 'The size of data is not equal'\n",
        "\n",
        "print(train_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcoy30OC3UGf",
        "outputId": "0f0ce85c-6d77-4e55-ef89-f916b5a438ee"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.30760095011876487, 0.23871733966745842, 0.29453681710213775, 0.38836104513064135, 0.47268408551068886, 0.35510688836104515, 0.3016627078384798, 0.34441805225653205, 0.3669833729216152, 0.45486935866983375, 0.48931116389548696, 0.23990498812351543, 0.5831353919239906, 0.40498812351543945, 0.2529691211401425, 0.3087885985748218, 0.38954869358669836, 0.47862232779097386, 0.4358669833729216, 0.27909738717339666, 0.3859857482185273, 0.19833729216152018, 0.7327790973871734, 0.9085510688836105, 0.9275534441805225, 1.0, 0.2850356294536817]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Neural Network Model</h1>"
      ],
      "metadata": {
        "id": "pSk5ZsP5bBjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainingGuardCallback(keras.callbacks.Callback):\n",
        "\n",
        "  def on_train_batch_end(self, batch, logs=None):\n",
        "    pass\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if logs.get('accuracy') is not None:\n",
        "      if epoch > 100 or logs.get('accuracy') > 0.95:\n",
        "        self.model.stop_training = True\n",
        "        print(\"\\nTraining goals met. Training has been stopped!!\")   "
      ],
      "metadata": {
        "id": "KsftvV5K-_au"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape = np.shape(train_data[0])))\n",
        "model.add(keras.layers.Dropout(0.2))\n",
        "model.add(keras.layers.Dense(1024, activation=keras.activations.relu)) # 1st hidden layer\n",
        "model.add(keras.layers.Dropout(0.3) ) # helps prevent overfitting\n",
        "model.add(keras.layers.Dense(2048, activation=keras.activations.relu)) # 2nd hidden layer\n",
        "model.add(keras.layers.Dense(14, activation=keras.activations.softmax)) # output layer"
      ],
      "metadata": {
        "id": "mMqx2AF8Q_aC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(                                          # use adam optimizer\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    metrics=['accuracy'], \n",
        "    loss=keras.losses.SparseCategoricalCrossentropy()   # labels provided as integers, for one-hot vectors use keras.losses.CategoricalCrossentropy\n",
        "    )"
      ],
      "metadata": {
        "id": "I6BIhUVuRcbf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_data, train_labels, verbose=1, epochs=999, callbacks=[TrainingGuardCallback()]) # train the model"
      ],
      "metadata": {
        "id": "RJHw_9EqRfXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"NN accuracy: {:.2f}% \\t loss: {:.5f}\".format(history.history['accuracy'][-1]*100, history.history['loss'][-1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GK1J48VhRhSH",
        "outputId": "09d1246e-d6d4-4135-ae5c-a66f2e69b310"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NN accuracy: 75.00% \t loss: 1.15231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Testing the model</h1>"
      ],
      "metadata": {
        "id": "W1kI05KpRmVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(test_data, test_labels, verbose=0)\n",
        "print(\"Model loss (test data): {:.3f} \\t Model accuracy (test data): {:.2f}%\".format(loss, accuracy*100))"
      ],
      "metadata": {
        "id": "1DooIXZeRsBl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0eeb6a3-64da-4e5c-febe-872785cf6f9e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loss (test data): 2.150 \t Model accuracy (test data): 28.57%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Data augmentation</h1>\n"
      ],
      "metadata": {
        "id": "rjri7reIz0f6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random as r\n",
        "\n",
        "def data_augmentation(set, **kwargs):\n",
        "  aug_set = Set(set.id, [], True)\n",
        "  valid = False\n",
        "\n",
        "  # random Method\n",
        "  if 'random' in kwargs:\n",
        "    valid = True\n",
        "    for i in range(0, len(set.keystrokes)):\n",
        "      r1 = math.floor((r.random() * 100) - 50)\n",
        "      r2 = math.floor((r.random() * 100) - 50)\n",
        "      keystroke = Keystroke(set.keystrokes[i].key, \n",
        "                            math.floor(set.keystrokes[i].up + r1),#dst[i]),\n",
        "                            math.floor(set.keystrokes[i].down + r2),# dst[i]),\n",
        "                            math.floor(set.keystrokes[i].down + r2) - math.floor(set.keystrokes[i].up + r1))#dst[i]) - math.floor(set.keystrokes[i].up + dst[i]))\n",
        "      \n",
        "      aug_set.keystrokes.append(keystroke)\n",
        "\n",
        "  # Gaussian dst Method\n",
        "  if 'gaussian_rand' in kwargs:\n",
        "    valid = True\n",
        "    if 'mu' not in kwargs:\n",
        "        mu = 25 # mean\n",
        "    if 'sigma' not in kwargs:\n",
        "        sigma = 50; # standard deviation\n",
        "\n",
        "    dst = np.random.normal(mu, sigma, len(set.keystrokes))\n",
        "\n",
        "    for i in range(0, len(set.keystrokes)):\n",
        "      keystroke = Keystroke(set.keystrokes[i].key, \n",
        "                            math.floor(set.keystrokes[i].up + dst[i]),\n",
        "                            math.floor(set.keystrokes[i].down + dst[i]),\n",
        "                            math.floor(set.keystrokes[i].down + dst[i]) - math.floor(set.keystrokes[i].up + dst[i]))\n",
        "      \n",
        "      aug_set.keystrokes.append(keystroke)\n",
        "\n",
        "  # Replace random samples Method\n",
        "  if 'random_replace' in kwargs:\n",
        "    valid = True\n",
        "\n",
        "    if 'replace_rate' not in kwargs:\n",
        "      rate = 10\n",
        "\n",
        "    next = rate\n",
        "    for i in range(len(set.keystrokes)):\n",
        "      if i == next:\n",
        "        index = i - math.ceil(r.random() * next)\n",
        "        index = 0 if index < 0 else index\n",
        "        sub = aug_set.keystrokes[index]\n",
        "\n",
        "        keystroke = Keystroke(set.keystrokes[i].key, sub.up, sub.down, sub.dtime)\n",
        "        keystrokeSub = Keystroke(sub.key, set.keystrokes[i].up, set.keystrokes[i].down, set.keystrokes[i].dtime)\n",
        "\n",
        "        aug_set.keystrokes[index] = keystrokeSub\n",
        "        aug_set.keystrokes.append(keystroke)\n",
        "        \n",
        "        next += rate\n",
        "      else:\n",
        "        aug_set.keystrokes.append(set.keystrokes[i])\n",
        "\n",
        "  # None of the above was specified\n",
        "  if valid is False:\n",
        "    raise Exception('Augmentation method not chosen')\n",
        "\n",
        "  return aug_set"
      ],
      "metadata": {
        "id": "rFuPDqq-z5cc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for key in dataset:\n",
        "  for i in range(len(dataset[key])):\n",
        "    aug = data_augmentation(dataset[key][i], random=True, gaussian_rand=True, random_replace=True)\n",
        "    dataset[key].insert(0, aug) # prepend\n",
        "  \n",
        "tdata, tlabels, testd, testl = convert_all(dataset)\n",
        "\n",
        "tdata += train_data\n",
        "tlabels += train_labels\n",
        "\n",
        "print('Augmented dataset size: ', len(tdata))\n",
        "assert len(tlabels) == len(tdata), 'Dataset corrupted'\n",
        "\n",
        "print(\"format: \", testd[0])"
      ],
      "metadata": {
        "id": "OvWAsoav2rDo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ba3049e-9cd8-437d-821f-bd4fe2157eec"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmented dataset size:  182\n",
            "format:  [0.42596810933940776, 0.21981776765375854, 0.49430523917995445, 0.2517084282460137, 0.31890660592255127, 0.3211845102505695, 0.1252847380410023, 0.7164009111617312, 0.40774487471526194, 0.4202733485193622, 0.30751708428246016, 0.23690205011389523, 0.23234624145785876, 0.5091116173120729, 0.31662870159453305, 0.7198177676537585, 0.2984054669703872, 0.3678815489749431, 0.30751708428246016, 0.5216400911161732, 0.10933940774487472, 0.36332574031890663, 0.47038724373576307, 0.7323462414578588, 0.06719817767653759, 1.0, 0.1947608200455581]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "i2utpx7FadOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Retrain the model with augmented data</h1>\n"
      ],
      "metadata": {
        "id": "dA655ewmaSWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(tdata, tlabels, verbose=1, epochs=200, shuffle=True, callbacks=[TrainingGuardCallback()])"
      ],
      "metadata": {
        "id": "VA3LE1c57_ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(testd, testl, verbose=0)\n",
        "print(\"Model loss (test data): {:.3f} \\t Model accuracy (test data): {:.2f}%\".format(loss, accuracy*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFvsEVQP8KsI",
        "outputId": "aa9b7cbe-e98a-4ca2-8dcb-6e440044ba01"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loss (test data): 1.937 \t Model accuracy (test data): 42.86%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Save augmented sets</h1>"
      ],
      "metadata": {
        "id": "WPk3sUdcasA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def save_aug_sets(dataset, path, offset):\n",
        "  os.makedirs(path, exist_ok=True) # create dir if not exists\n",
        "\n",
        "  for key in dataset:\n",
        "    sampleid = offset\n",
        "\n",
        "    for set in dataset[key]:\n",
        "      if set.aug is not True:\n",
        "        continue\n",
        "\n",
        "      filepath = path + \"/#\" + str(set.id+1).zfill(2) + \"_\" + str(sampleid)  + \".txt\"\n",
        "      sampleid += 1\n",
        "      \n",
        "      with open(filepath, 'w') as f:\n",
        "\n",
        "        for k in set.keystrokes:\n",
        "          f.write(\"\\t{}, \\t{}, \\t{}\".format(k.key, k.down, k.up))\n",
        "          f.write('\\n')"
      ],
      "metadata": {
        "id": "gmNErS0-IM_7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_aug_sets(dataset, \"drive/MyDrive/Keystrokes/Augmented\", 3)"
      ],
      "metadata": {
        "id": "9soQFCsVKiHE"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}