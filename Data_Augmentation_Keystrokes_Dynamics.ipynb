{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Augmentation - Keystrokes Dynamics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Ka30FJh0QAnzPokn8tqtF0cfJb9a8Qbt",
      "authorship_tag": "ABX9TyOP7EM3zQ7tGrkB/NfzRzLD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheYoungBeast/Data-Augmentation-Keystrokes-Dynamics/blob/main/Data_Augmentation_Keystrokes_Dynamics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iS9qIUysQ1jR",
        "outputId": "736c5344-16e5-4c0d-d700-de44bab65973"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from numpy.ma.core import array"
      ],
      "metadata": {
        "id": "XFbsJeKD6jVX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import NamedTuple\n",
        "\n",
        "class Set(NamedTuple):\n",
        "  id: int\n",
        "  keystrokes: array = []\n",
        "  aug: bool = False\n",
        "\n",
        "class Keystroke(NamedTuple):\n",
        "  key: str\n",
        "  up: int\n",
        "  down: int\n",
        "  dtime: int"
      ],
      "metadata": {
        "id": "E4XG8mMmYVmA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = {};\n",
        "\n",
        "for i in range(1, 15):\n",
        "  dataset[str(i)] = []\n",
        "\n",
        "  for j in range(0, 3):\n",
        "    filepath = \"drive/MyDrive/Keystrokes/#\" + str(i).zfill(2) + \"_\" + str(j)  +\".txt\"\n",
        "    dataset[str(i)].append(Set(i-1, []))\n",
        "\n",
        "    with open(filepath, 'r') as f:\n",
        "            line = f.readline()\n",
        "            while line:\n",
        "\n",
        "              if len(line.strip().split(',')) == 3:\n",
        "                key, down, up = line.strip().split(',')\n",
        "                keystroke = Keystroke(key, int(up), int(down), int(down) - int(up))\n",
        "                dataset[str(i)][-1].keystrokes.append(keystroke)\n",
        "\n",
        "              line = f.readline()"
      ],
      "metadata": {
        "id": "m-H1GaSiSkLY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#def convert_to_datasets(dataset):\n",
        "#  train_data = []\n",
        "#  test_data = []\n",
        "#  train_labels = []\n",
        "#  test_labels = []\n",
        "#\n",
        "#  for key in dataset:\n",
        "#    for i in range(0, len(dataset[key])-1):\n",
        "#      train_data.append([])\n",
        "#      for k in dataset[key][i].keystrokes:\n",
        "#        train_data[-1].append(k.dtime)\n",
        "#\n",
        "#      train_labels.append(dataset[key][i].id)\n",
        "#    \n",
        "#    test_data.append([])\n",
        "#    for k in dataset[key][-1].keystrokes:\n",
        "#      test_data[-1].append(k.dtime)\n",
        "#\n",
        "#    test_labels.append(dataset[key][-1].id)\n",
        "#\n",
        "#  return train_data, train_labels, test_data, test_labels"
      ],
      "metadata": {
        "id": "PBhRhE9wc5Me"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(set):\n",
        "  dict = {}\n",
        "  count = {}\n",
        "\n",
        "  for keystroke in set.keystrokes:\n",
        "    if not keystroke.key:\n",
        "      continue\n",
        "\n",
        "    if keystroke.key not in dict:\n",
        "      dict[keystroke.key] = 0\n",
        "      count[keystroke.key] = 0\n",
        "\n",
        "    dict[keystroke.key] += abs(keystroke.dtime) # Absolute value, dwell time\n",
        "    count[keystroke.key] += 1\n",
        "\n",
        "  # Average dwell time per Key\n",
        "  avgs = []\n",
        "  \n",
        "  for key in dict:\n",
        "    avgs.append(math.floor(dict[key]/count[key]))\n",
        "\n",
        "  if len(avgs) is not 27:\n",
        "    raise Exception('Dataset is corrupted', 'The number of unique keys in the set exceeds 27')\n",
        "\n",
        "  return avgs, set.id"
      ],
      "metadata": {
        "id": "4RpNBKn55OgO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(data):\n",
        "  norm_data = []\n",
        "  max = np.max(data)\n",
        "\n",
        "  for i in range(len(data)):\n",
        "    norm_data.append(data[i] / max) # local normalization\n",
        "\n",
        "  return norm_data"
      ],
      "metadata": {
        "id": "8_vjuHxE-PFD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_all(dataset):\n",
        "  train_data = []\n",
        "  test_data = []\n",
        "  train_labels = []\n",
        "  test_labels = []\n",
        "\n",
        "  for key in dataset:\n",
        "    for i in range(0, len(dataset[key])-1): # skip last set\n",
        "      tdata, sid = preprocess(dataset[key][i])\n",
        "\n",
        "      train_data.append(normalize(tdata)) # normalization\n",
        "      train_labels.append(sid)\n",
        "    \n",
        "    tdata, sid = preprocess(dataset[key][-1]) # treat last set as test data\n",
        "    test_data.append(normalize(tdata)) # normalizaton\n",
        "    test_labels.append(sid)\n",
        "\n",
        "  return train_data, train_labels, test_data, test_labels"
      ],
      "metadata": {
        "id": "r7EOKCwH-RxL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, train_labels, test_data, test_labels = convert_all(dataset)\n",
        "\n",
        "assert len(test_data) == len(test_labels), 'The size of data is not equal'\n",
        "assert len(train_data) == len(train_labels), 'The size of data is not equal'\n",
        "\n",
        "print(train_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcoy30OC3UGf",
        "outputId": "0f0ce85c-6d77-4e55-ef89-f916b5a438ee"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.30760095011876487, 0.23871733966745842, 0.29453681710213775, 0.38836104513064135, 0.47268408551068886, 0.35510688836104515, 0.3016627078384798, 0.34441805225653205, 0.3669833729216152, 0.45486935866983375, 0.48931116389548696, 0.23990498812351543, 0.5831353919239906, 0.40498812351543945, 0.2529691211401425, 0.3087885985748218, 0.38954869358669836, 0.47862232779097386, 0.4358669833729216, 0.27909738717339666, 0.3859857482185273, 0.19833729216152018, 0.7327790973871734, 0.9085510688836105, 0.9275534441805225, 1.0, 0.2850356294536817]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainingGuardCallback(keras.callbacks.Callback):\n",
        "\n",
        "  def on_train_batch_end(self, batch, logs=None):\n",
        "    pass\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if logs.get('accuracy') is not None:\n",
        "      if epoch > 100 or logs.get('accuracy') > 0.95:\n",
        "        self.model.stop_training = True\n",
        "        print(\"\\nTraining goals met. Training has been stopped!!\")   "
      ],
      "metadata": {
        "id": "KsftvV5K-_au"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape = np.shape(train_data[0])))\n",
        "model.add(keras.layers.Dropout(0.2))\n",
        "model.add(keras.layers.Dense(1024, activation=keras.activations.relu)) # 1st hidden layer\n",
        "model.add(keras.layers.Dropout(0.3) ) # helps prevent overfitting\n",
        "model.add(keras.layers.Dense(2048, activation=keras.activations.relu)) # 2nd hidden layer\n",
        "model.add(keras.layers.Dense(14, activation=keras.activations.softmax)) # output layer"
      ],
      "metadata": {
        "id": "mMqx2AF8Q_aC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(                                          # use default rmsprop optimizer\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    metrics=['accuracy'], \n",
        "    loss=keras.losses.SparseCategoricalCrossentropy()   # labels provided as integers, for one-hot vectors use keras.losses.CategoricalCrossentropy\n",
        "    )"
      ],
      "metadata": {
        "id": "I6BIhUVuRcbf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_data, train_labels, verbose=1, epochs=999, callbacks=[TrainingGuardCallback()]) # train the model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJHw_9EqRfXY",
        "outputId": "d59f1e68-5aee-481a-fbcb-1085db7c9d56"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/999\n",
            "1/1 [==============================] - 1s 518ms/step - loss: 2.6680 - accuracy: 0.0714\n",
            "Epoch 2/999\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 2.5408 - accuracy: 0.2857\n",
            "Epoch 3/999\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 2.5167 - accuracy: 0.2500\n",
            "Epoch 4/999\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 2.4581 - accuracy: 0.2500\n",
            "Epoch 5/999\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 2.3790 - accuracy: 0.2857\n",
            "Epoch 6/999\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 2.2810 - accuracy: 0.3929\n",
            "Epoch 7/999\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 2.2512 - accuracy: 0.2857\n",
            "Epoch 8/999\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 2.1842 - accuracy: 0.3929\n",
            "Epoch 9/999\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.9886 - accuracy: 0.5000\n",
            "Epoch 10/999\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 2.0208 - accuracy: 0.5000\n",
            "Epoch 11/999\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.9329 - accuracy: 0.5357\n",
            "Epoch 12/999\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.8234 - accuracy: 0.5714\n",
            "Epoch 13/999\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.8776 - accuracy: 0.5000\n",
            "Epoch 14/999\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.5786 - accuracy: 0.7500\n",
            "Epoch 15/999\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.5494 - accuracy: 0.6071\n",
            "Epoch 16/999\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 1.5201 - accuracy: 0.5714\n",
            "Epoch 17/999\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.6009 - accuracy: 0.5357\n",
            "Epoch 18/999\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.4116 - accuracy: 0.6429\n",
            "Epoch 19/999\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.3819 - accuracy: 0.5357\n",
            "Epoch 20/999\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1.1659 - accuracy: 0.7500\n",
            "Epoch 21/999\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.0973 - accuracy: 0.7143\n",
            "Epoch 22/999\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1.2780 - accuracy: 0.6786\n",
            "Epoch 23/999\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1.2143 - accuracy: 0.6429\n",
            "Epoch 24/999\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.1726 - accuracy: 0.6429\n",
            "Epoch 25/999\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.0263 - accuracy: 0.7143\n",
            "Epoch 26/999\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.5782 - accuracy: 0.4286\n",
            "Epoch 27/999\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.3549 - accuracy: 0.5357\n",
            "Epoch 28/999\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.8635 - accuracy: 0.6429\n",
            "Epoch 29/999\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1.2055 - accuracy: 0.6429\n",
            "Epoch 30/999\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.9853 - accuracy: 0.7143\n",
            "Epoch 31/999\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.8631 - accuracy: 0.6786\n",
            "Epoch 32/999\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1.1266 - accuracy: 0.6786\n",
            "Epoch 33/999\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1.0058 - accuracy: 0.6071\n",
            "Epoch 34/999\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.6099 - accuracy: 0.4643\n",
            "Epoch 35/999\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.9921 - accuracy: 0.6429\n",
            "Epoch 36/999\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 1.1729 - accuracy: 0.6429\n",
            "Epoch 37/999\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.8805 - accuracy: 0.7500\n",
            "Epoch 38/999\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.0784 - accuracy: 0.6786\n",
            "Epoch 39/999\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.8315 - accuracy: 0.6786\n",
            "Epoch 40/999\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.7939 - accuracy: 0.7143\n",
            "Epoch 41/999\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.0360 - accuracy: 0.6071\n",
            "Epoch 42/999\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.0011 - accuracy: 0.7500\n",
            "Epoch 43/999\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 1.1445 - accuracy: 0.6429\n",
            "Epoch 44/999\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.8029 - accuracy: 0.7857\n",
            "Epoch 45/999\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6270 - accuracy: 0.8214\n",
            "Epoch 46/999\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.1047 - accuracy: 0.6429\n",
            "Epoch 47/999\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.8955 - accuracy: 0.7143\n",
            "Epoch 48/999\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7427 - accuracy: 0.7500\n",
            "Epoch 49/999\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.6139 - accuracy: 0.8571\n",
            "Epoch 50/999\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.9455 - accuracy: 0.6429\n",
            "Epoch 51/999\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 1.2609 - accuracy: 0.6071\n",
            "Epoch 52/999\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.0066 - accuracy: 0.6071\n",
            "Epoch 53/999\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.4979 - accuracy: 0.8571\n",
            "Epoch 54/999\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.5044 - accuracy: 0.8929\n",
            "Epoch 55/999\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.9245 - accuracy: 0.6786\n",
            "Epoch 56/999\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.8230 - accuracy: 0.7143\n",
            "Epoch 57/999\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.7615 - accuracy: 0.7500\n",
            "Epoch 58/999\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.8204 - accuracy: 0.7857\n",
            "Epoch 59/999\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1.4704 - accuracy: 0.6071\n",
            "Epoch 60/999\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.7191 - accuracy: 0.6786\n",
            "Epoch 61/999\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1.1930 - accuracy: 0.5714\n",
            "Epoch 62/999\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.7108 - accuracy: 0.7500\n",
            "Epoch 63/999\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.9438 - accuracy: 0.7143\n",
            "Epoch 64/999\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.9834 - accuracy: 0.6071\n",
            "Epoch 65/999\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.8072 - accuracy: 0.7143\n",
            "Epoch 66/999\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.5947 - accuracy: 0.7500\n",
            "Epoch 67/999\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 1.1881 - accuracy: 0.5714\n",
            "Epoch 68/999\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 1.3847 - accuracy: 0.5714\n",
            "Epoch 69/999\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.9305 - accuracy: 0.7143\n",
            "Epoch 70/999\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.8860 - accuracy: 0.6786\n",
            "Epoch 71/999\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.8914 - accuracy: 0.7500\n",
            "Epoch 72/999\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.8479 - accuracy: 0.7857\n",
            "Epoch 73/999\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7877 - accuracy: 0.7500\n",
            "Epoch 74/999\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 1.1374 - accuracy: 0.5714\n",
            "Epoch 75/999\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.6871 - accuracy: 0.7857\n",
            "Epoch 76/999\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 1.0841 - accuracy: 0.6071\n",
            "Epoch 77/999\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 1.0303 - accuracy: 0.6071\n",
            "Epoch 78/999\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.6479 - accuracy: 0.7857\n",
            "Epoch 79/999\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7692 - accuracy: 0.7500\n",
            "Epoch 80/999\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.0303 - accuracy: 0.6786\n",
            "Epoch 81/999\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.8796 - accuracy: 0.7500\n",
            "Epoch 82/999\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.6498 - accuracy: 0.8571\n",
            "Epoch 83/999\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 1.0402 - accuracy: 0.7143\n",
            "Epoch 84/999\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.6360 - accuracy: 0.7500\n",
            "Epoch 85/999\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.9302 - accuracy: 0.6071\n",
            "Epoch 86/999\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.8502 - accuracy: 0.7500\n",
            "Epoch 87/999\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.8592 - accuracy: 0.7500\n",
            "Epoch 88/999\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.8628 - accuracy: 0.7500\n",
            "Epoch 89/999\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7191 - accuracy: 0.7500\n",
            "Epoch 90/999\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1.3672 - accuracy: 0.4643\n",
            "Epoch 91/999\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1.0575 - accuracy: 0.6429\n",
            "Epoch 92/999\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.6628 - accuracy: 0.7143\n",
            "Epoch 93/999\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.1925 - accuracy: 0.5357\n",
            "Epoch 94/999\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.6316 - accuracy: 0.7857\n",
            "Epoch 95/999\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.8078 - accuracy: 0.7857\n",
            "Epoch 96/999\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.8848 - accuracy: 0.7143\n",
            "Epoch 97/999\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.5864 - accuracy: 0.8929\n",
            "Epoch 98/999\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.6120 - accuracy: 0.8214\n",
            "Epoch 99/999\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.6176 - accuracy: 0.8214\n",
            "Epoch 100/999\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.3613 - accuracy: 0.9286\n",
            "Epoch 101/999\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.7012 - accuracy: 0.7143\n",
            "Epoch 102/999\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.1523 - accuracy: 0.7500\n",
            "Training goals met. Training has been stopped!!\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 1.1523 - accuracy: 0.7500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"NN accuracy: {:.2f}% \\t loss: {:.5f}\".format(history.history['accuracy'][-1]*100, history.history['loss'][-1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GK1J48VhRhSH",
        "outputId": "09d1246e-d6d4-4135-ae5c-a66f2e69b310"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NN accuracy: 75.00% \t loss: 1.15231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Testing the model</h1>"
      ],
      "metadata": {
        "id": "W1kI05KpRmVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(test_data, test_labels, verbose=0)\n",
        "print(\"Model loss (test data): {:.3f} \\t Model accuracy (test data): {:.2f}%\".format(loss, accuracy*100))"
      ],
      "metadata": {
        "id": "1DooIXZeRsBl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0eeb6a3-64da-4e5c-febe-872785cf6f9e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loss (test data): 2.150 \t Model accuracy (test data): 28.57%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Data augmentation</h1>\n"
      ],
      "metadata": {
        "id": "rjri7reIz0f6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random as r\n",
        "\n",
        "def data_augmentation(set, **kwargs):\n",
        "  aug_set = Set(set.id, [], True)\n",
        "  valid = False\n",
        "\n",
        "  # random Method\n",
        "  if 'random' in kwargs:\n",
        "    valid = True\n",
        "    for i in range(0, len(set.keystrokes)):\n",
        "      r1 = math.floor((r.random() * 100) - 50)\n",
        "      r2 = math.floor((r.random() * 100) - 50)\n",
        "      keystroke = Keystroke(set.keystrokes[i].key, \n",
        "                            math.floor(set.keystrokes[i].up + r1),#dst[i]),\n",
        "                            math.floor(set.keystrokes[i].down + r2),# dst[i]),\n",
        "                            math.floor(set.keystrokes[i].down + r2) - math.floor(set.keystrokes[i].up + r1))#dst[i]) - math.floor(set.keystrokes[i].up + dst[i]))\n",
        "      \n",
        "      aug_set.keystrokes.append(keystroke)\n",
        "\n",
        "  # Gaussian dst Method\n",
        "  if 'gaussian_rand' in kwargs:\n",
        "    valid = True\n",
        "    if 'mu' not in kwargs:\n",
        "        mu = 25 # mean\n",
        "    if 'sigma' not in kwargs:\n",
        "        sigma = 50; # standard deviation\n",
        "\n",
        "    dst = np.random.normal(mu, sigma, len(set.keystrokes))\n",
        "\n",
        "    for i in range(0, len(set.keystrokes)):\n",
        "      keystroke = Keystroke(set.keystrokes[i].key, \n",
        "                            math.floor(set.keystrokes[i].up + dst[i]),\n",
        "                            math.floor(set.keystrokes[i].down + dst[i]),\n",
        "                            math.floor(set.keystrokes[i].down + dst[i]) - math.floor(set.keystrokes[i].up + dst[i]))\n",
        "      \n",
        "      aug_set.keystrokes.append(keystroke)\n",
        "\n",
        "  # Replace random samples Method\n",
        "  if 'random_replace' in kwargs:\n",
        "    valid = True\n",
        "\n",
        "    if 'replace_rate' not in kwargs:\n",
        "      rate = 10\n",
        "\n",
        "    next = rate\n",
        "    for i in range(len(set.keystrokes)):\n",
        "      if i == next:\n",
        "        index = i - math.ceil(r.random() * next)\n",
        "        index = 0 if index < 0 else index\n",
        "        sub = aug_set.keystrokes[index]\n",
        "\n",
        "        keystroke = Keystroke(set.keystrokes[i].key, sub.up, sub.down, sub.dtime)\n",
        "        keystrokeSub = Keystroke(sub.key, set.keystrokes[i].up, set.keystrokes[i].down, set.keystrokes[i].dtime)\n",
        "\n",
        "        aug_set.keystrokes[index] = keystrokeSub\n",
        "        aug_set.keystrokes.append(keystroke)\n",
        "        \n",
        "        next += rate\n",
        "      else:\n",
        "        aug_set.keystrokes.append(set.keystrokes[i])\n",
        "\n",
        "  # None of the above was specified\n",
        "  if valid is False:\n",
        "    raise Exception('Augmentation method not chosen')\n",
        "\n",
        "  return aug_set"
      ],
      "metadata": {
        "id": "rFuPDqq-z5cc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for key in dataset:\n",
        "  for i in range(len(dataset[key])):\n",
        "    aug = data_augmentation(dataset[key][i], random=True, gaussian_rand=True, random_replace=True)\n",
        "    dataset[key].insert(0, aug) # prepend\n",
        "  \n",
        "tdata, tlabels, testd, testl = convert_all(dataset)\n",
        "\n",
        "tdata += train_data\n",
        "tlabels += train_labels\n",
        "\n",
        "print(len(tdata))\n",
        "print(len(tlabels))\n",
        "\n",
        "print(len(testd), testd)\n",
        "print(len(test_data), test_data)"
      ],
      "metadata": {
        "id": "OvWAsoav2rDo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79225c16-080a-4949-cdb2-df6788cd5c37"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "98\n",
            "98\n",
            "14 [[0.42596810933940776, 0.21981776765375854, 0.49430523917995445, 0.2517084282460137, 0.31890660592255127, 0.3211845102505695, 0.1252847380410023, 0.7164009111617312, 0.40774487471526194, 0.4202733485193622, 0.30751708428246016, 0.23690205011389523, 0.23234624145785876, 0.5091116173120729, 0.31662870159453305, 0.7198177676537585, 0.2984054669703872, 0.3678815489749431, 0.30751708428246016, 0.5216400911161732, 0.10933940774487472, 0.36332574031890663, 0.47038724373576307, 0.7323462414578588, 0.06719817767653759, 1.0, 0.1947608200455581], [0.35811648079306074, 0.436183395291202, 0.43742255266418834, 0.5600991325898389, 0.5749690210656754, 0.3754646840148699, 0.20693928128872366, 0.41511771995043373, 0.6059479553903345, 0.44609665427509293, 0.49318463444857497, 0.38909541511771994, 0.4225526641883519, 0.2936802973977695, 0.7236679058240396, 0.3630731102850062, 0.49814126394052044, 0.355638166047088, 0.4634448574969021, 0.5216852540272615, 1.0, 0.5291201982651796, 0.5105328376703842, 0.38042131350681535, 0.07187112763320942, 0.5464684014869888, 0.8574969021065675], [0.566066066066066, 0.44144144144144143, 0.5405405405405406, 0.545045045045045, 0.6681681681681682, 0.515015015015015, 0.487987987987988, 1.0, 0.6051051051051051, 0.6621621621621622, 0.3303303303303303, 0.42042042042042044, 0.7192192192192193, 0.5075075075075075, 0.5990990990990991, 0.7012012012012012, 0.484984984984985, 0.7072072072072072, 0.6336336336336337, 0.6531531531531531, 0.6426426426426426, 0.8963963963963963, 0.6006006006006006, 0.46996996996997, 0.7612612612612613, 0.5765765765765766, 0.13663663663663664], [0.3553370786516854, 0.2949438202247191, 0.5337078651685393, 0.5280898876404494, 0.42837078651685395, 0.3890449438202247, 0.3595505617977528, 0.6558988764044944, 0.449438202247191, 0.41292134831460675, 0.4747191011235955, 0.18117977528089887, 0.29634831460674155, 0.27247191011235955, 0.39747191011235955, 0.7598314606741573, 0.32865168539325845, 0.29634831460674155, 0.3553370786516854, 0.6418539325842697, 1.0, 0.34129213483146065, 0.547752808988764, 0.4789325842696629, 0.3890449438202247, 0.8047752808988764, 0.574438202247191], [0.32269503546099293, 0.24822695035460993, 0.4444444444444444, 0.32505910165484636, 0.28368794326241137, 0.2990543735224586, 0.3605200945626478, 0.5520094562647754, 0.2115839243498818, 0.3557919621749409, 0.1359338061465721, 0.14420803782505912, 0.18557919621749408, 0.3262411347517731, 0.1430260047281324, 0.15602836879432624, 0.19976359338061467, 0.3735224586288416, 0.2706855791962175, 0.35224586288416077, 1.0, 0.1595744680851064, 0.42671394799054374, 0.21040189125295508, 0.3132387706855792, 0.2695035460992908, 0.2635933806146572], [0.49157733537519144, 0.48545176110260335, 0.8223583460949464, 0.5436447166921899, 0.4318529862174579, 0.4333843797856049, 0.5160796324655437, 0.5987748851454824, 0.7274119448698315, 0.5742725880551302, 0.7381316998468607, 0.37978560490045943, 0.7059724349157733, 0.8085758039816233, 0.3001531393568147, 0.7396630934150077, 0.663093415007657, 0.5727411944869831, 0.6003062787136294, 0.47320061255742724, 0.667687595712098, 0.6125574272588055, 0.8422664624808576, 1.0, 0.25880551301684535, 0.20673813169984687, 0.2894333843797856], [1.0, 0.4676724137931034, 0.5991379310344828, 0.42887931034482757, 0.521551724137931, 0.5474137931034483, 0.12931034482758622, 0.6077586206896551, 0.5689655172413793, 0.44396551724137934, 0.5064655172413793, 0.21120689655172414, 0.3448275862068966, 0.646551724137931, 0.27586206896551724, 0.30603448275862066, 0.6400862068965517, 0.4956896551724138, 0.33836206896551724, 0.40301724137931033, 0.4267241379310345, 0.43103448275862066, 0.08836206896551724, 0.5301724137931034, 0.6508620689655172, 0.584051724137931, 0.14870689655172414], [0.6440366972477064, 0.24403669724770644, 0.2036697247706422, 0.163302752293578, 0.1889908256880734, 0.13211009174311927, 0.09724770642201835, 0.24587155963302754, 0.22568807339449543, 0.1944954128440367, 0.13211009174311927, 0.08073394495412844, 0.09724770642201835, 0.1688073394495413, 0.10458715596330276, 0.1651376146788991, 0.07155963302752294, 0.07522935779816514, 0.14128440366972478, 0.1908256880733945, 1.0, 0.1743119266055046, 0.26422018348623855, 0.48440366972477067, 0.03302752293577982, 0.2, 0.1853211009174312], [0.26842105263157895, 0.24210526315789474, 0.3105263157894737, 0.28421052631578947, 0.48596491228070177, 0.2771929824561403, 0.631578947368421, 0.32280701754385965, 0.23859649122807017, 0.4964912280701754, 0.32280701754385965, 0.2, 0.28771929824561404, 0.3368421052631579, 0.3157894736842105, 0.21754385964912282, 0.29473684210526313, 0.43157894736842106, 0.20877192982456141, 0.4070175438596491, 0.16666666666666666, 0.3140350877192982, 0.15263157894736842, 0.6350877192982456, 0.07719298245614035, 1.0, 0.3912280701754386], [0.5006134969325153, 0.2674846625766871, 0.3079754601226994, 0.2920245398773006, 0.49447852760736194, 0.37668711656441717, 0.18159509202453988, 0.6588957055214724, 0.5938650306748466, 0.4601226993865031, 0.6748466257668712, 0.3423312883435583, 0.252760736196319, 0.49325153374233127, 0.2687116564417178, 0.7742331288343558, 0.37668711656441717, 0.49079754601226994, 0.37791411042944784, 0.4245398773006135, 1.0, 0.3914110429447853, 0.38650306748466257, 0.2196319018404908, 0.6625766871165644, 0.2552147239263804, 0.4233128834355828], [0.4236526946107784, 0.4341317365269461, 0.5838323353293413, 0.33532934131736525, 0.5284431137724551, 0.30988023952095806, 0.405688622754491, 0.8862275449101796, 0.3413173652694611, 0.46706586826347307, 0.2994011976047904, 0.2155688622754491, 0.21407185628742514, 0.47305389221556887, 0.25449101796407186, 0.5508982035928144, 0.30538922155688625, 0.3323353293413174, 0.24251497005988024, 0.6182634730538922, 0.8637724550898204, 0.23203592814371257, 0.3398203592814371, 1.0, 0.7485029940119761, 0.35479041916167664, 0.19760479041916168], [0.7293906810035843, 0.3727598566308244, 0.5232974910394266, 0.478494623655914, 0.5304659498207885, 0.4050179211469534, 0.8028673835125448, 0.5591397849462365, 0.5501792114695341, 0.6523297491039427, 0.34587813620071683, 0.24551971326164876, 0.4157706093189964, 0.7114695340501792, 0.33154121863799285, 0.47491039426523296, 0.4050179211469534, 0.5913978494623656, 0.3673835125448029, 0.6093189964157706, 0.41397849462365593, 0.45340501792114696, 0.2670250896057348, 1.0, 0.510752688172043, 0.1863799283154122, 0.32437275985663083], [0.5829081632653061, 0.2423469387755102, 0.7385204081632653, 0.3405612244897959, 0.5114795918367347, 0.2461734693877551, 0.12244897959183673, 0.7908163265306123, 0.3903061224489796, 0.3762755102040816, 0.5012755102040817, 0.3086734693877551, 0.26913265306122447, 0.3635204081632653, 0.27168367346938777, 0.1913265306122449, 0.2653061224489796, 0.3163265306122449, 0.34311224489795916, 0.3252551020408163, 0.8775510204081632, 0.25637755102040816, 0.45663265306122447, 1.0, 0.1913265306122449, 0.8788265306122449, 0.6377551020408163], [0.771004942339374, 0.4464579901153213, 0.8632619439868204, 0.6902800658978583, 0.48929159802306427, 0.49093904448105435, 0.39373970345963755, 1.0, 0.48105436573311366, 0.6095551894563427, 0.8484349258649094, 0.5436573311367381, 0.514003294892916, 0.8401976935749588, 0.9522240527182867, 0.71499176276771, 0.5667215815485996, 0.3261943986820428, 0.8253706754530478, 0.729818780889621, 0.29983525535420097, 0.4135090609555189, 0.6688632619439868, 0.9373970345963756, 0.9390444810543658, 0.7116968698517299, 0.7084019769357496]]\n",
            "14 [[0.42596810933940776, 0.21981776765375854, 0.49430523917995445, 0.2517084282460137, 0.31890660592255127, 0.3211845102505695, 0.1252847380410023, 0.7164009111617312, 0.40774487471526194, 0.4202733485193622, 0.30751708428246016, 0.23690205011389523, 0.23234624145785876, 0.5091116173120729, 0.31662870159453305, 0.7198177676537585, 0.2984054669703872, 0.3678815489749431, 0.30751708428246016, 0.5216400911161732, 0.10933940774487472, 0.36332574031890663, 0.47038724373576307, 0.7323462414578588, 0.06719817767653759, 1.0, 0.1947608200455581], [0.35811648079306074, 0.436183395291202, 0.43742255266418834, 0.5600991325898389, 0.5749690210656754, 0.3754646840148699, 0.20693928128872366, 0.41511771995043373, 0.6059479553903345, 0.44609665427509293, 0.49318463444857497, 0.38909541511771994, 0.4225526641883519, 0.2936802973977695, 0.7236679058240396, 0.3630731102850062, 0.49814126394052044, 0.355638166047088, 0.4634448574969021, 0.5216852540272615, 1.0, 0.5291201982651796, 0.5105328376703842, 0.38042131350681535, 0.07187112763320942, 0.5464684014869888, 0.8574969021065675], [0.566066066066066, 0.44144144144144143, 0.5405405405405406, 0.545045045045045, 0.6681681681681682, 0.515015015015015, 0.487987987987988, 1.0, 0.6051051051051051, 0.6621621621621622, 0.3303303303303303, 0.42042042042042044, 0.7192192192192193, 0.5075075075075075, 0.5990990990990991, 0.7012012012012012, 0.484984984984985, 0.7072072072072072, 0.6336336336336337, 0.6531531531531531, 0.6426426426426426, 0.8963963963963963, 0.6006006006006006, 0.46996996996997, 0.7612612612612613, 0.5765765765765766, 0.13663663663663664], [0.3553370786516854, 0.2949438202247191, 0.5337078651685393, 0.5280898876404494, 0.42837078651685395, 0.3890449438202247, 0.3595505617977528, 0.6558988764044944, 0.449438202247191, 0.41292134831460675, 0.4747191011235955, 0.18117977528089887, 0.29634831460674155, 0.27247191011235955, 0.39747191011235955, 0.7598314606741573, 0.32865168539325845, 0.29634831460674155, 0.3553370786516854, 0.6418539325842697, 1.0, 0.34129213483146065, 0.547752808988764, 0.4789325842696629, 0.3890449438202247, 0.8047752808988764, 0.574438202247191], [0.32269503546099293, 0.24822695035460993, 0.4444444444444444, 0.32505910165484636, 0.28368794326241137, 0.2990543735224586, 0.3605200945626478, 0.5520094562647754, 0.2115839243498818, 0.3557919621749409, 0.1359338061465721, 0.14420803782505912, 0.18557919621749408, 0.3262411347517731, 0.1430260047281324, 0.15602836879432624, 0.19976359338061467, 0.3735224586288416, 0.2706855791962175, 0.35224586288416077, 1.0, 0.1595744680851064, 0.42671394799054374, 0.21040189125295508, 0.3132387706855792, 0.2695035460992908, 0.2635933806146572], [0.49157733537519144, 0.48545176110260335, 0.8223583460949464, 0.5436447166921899, 0.4318529862174579, 0.4333843797856049, 0.5160796324655437, 0.5987748851454824, 0.7274119448698315, 0.5742725880551302, 0.7381316998468607, 0.37978560490045943, 0.7059724349157733, 0.8085758039816233, 0.3001531393568147, 0.7396630934150077, 0.663093415007657, 0.5727411944869831, 0.6003062787136294, 0.47320061255742724, 0.667687595712098, 0.6125574272588055, 0.8422664624808576, 1.0, 0.25880551301684535, 0.20673813169984687, 0.2894333843797856], [1.0, 0.4676724137931034, 0.5991379310344828, 0.42887931034482757, 0.521551724137931, 0.5474137931034483, 0.12931034482758622, 0.6077586206896551, 0.5689655172413793, 0.44396551724137934, 0.5064655172413793, 0.21120689655172414, 0.3448275862068966, 0.646551724137931, 0.27586206896551724, 0.30603448275862066, 0.6400862068965517, 0.4956896551724138, 0.33836206896551724, 0.40301724137931033, 0.4267241379310345, 0.43103448275862066, 0.08836206896551724, 0.5301724137931034, 0.6508620689655172, 0.584051724137931, 0.14870689655172414], [0.6440366972477064, 0.24403669724770644, 0.2036697247706422, 0.163302752293578, 0.1889908256880734, 0.13211009174311927, 0.09724770642201835, 0.24587155963302754, 0.22568807339449543, 0.1944954128440367, 0.13211009174311927, 0.08073394495412844, 0.09724770642201835, 0.1688073394495413, 0.10458715596330276, 0.1651376146788991, 0.07155963302752294, 0.07522935779816514, 0.14128440366972478, 0.1908256880733945, 1.0, 0.1743119266055046, 0.26422018348623855, 0.48440366972477067, 0.03302752293577982, 0.2, 0.1853211009174312], [0.26842105263157895, 0.24210526315789474, 0.3105263157894737, 0.28421052631578947, 0.48596491228070177, 0.2771929824561403, 0.631578947368421, 0.32280701754385965, 0.23859649122807017, 0.4964912280701754, 0.32280701754385965, 0.2, 0.28771929824561404, 0.3368421052631579, 0.3157894736842105, 0.21754385964912282, 0.29473684210526313, 0.43157894736842106, 0.20877192982456141, 0.4070175438596491, 0.16666666666666666, 0.3140350877192982, 0.15263157894736842, 0.6350877192982456, 0.07719298245614035, 1.0, 0.3912280701754386], [0.5006134969325153, 0.2674846625766871, 0.3079754601226994, 0.2920245398773006, 0.49447852760736194, 0.37668711656441717, 0.18159509202453988, 0.6588957055214724, 0.5938650306748466, 0.4601226993865031, 0.6748466257668712, 0.3423312883435583, 0.252760736196319, 0.49325153374233127, 0.2687116564417178, 0.7742331288343558, 0.37668711656441717, 0.49079754601226994, 0.37791411042944784, 0.4245398773006135, 1.0, 0.3914110429447853, 0.38650306748466257, 0.2196319018404908, 0.6625766871165644, 0.2552147239263804, 0.4233128834355828], [0.4236526946107784, 0.4341317365269461, 0.5838323353293413, 0.33532934131736525, 0.5284431137724551, 0.30988023952095806, 0.405688622754491, 0.8862275449101796, 0.3413173652694611, 0.46706586826347307, 0.2994011976047904, 0.2155688622754491, 0.21407185628742514, 0.47305389221556887, 0.25449101796407186, 0.5508982035928144, 0.30538922155688625, 0.3323353293413174, 0.24251497005988024, 0.6182634730538922, 0.8637724550898204, 0.23203592814371257, 0.3398203592814371, 1.0, 0.7485029940119761, 0.35479041916167664, 0.19760479041916168], [0.7293906810035843, 0.3727598566308244, 0.5232974910394266, 0.478494623655914, 0.5304659498207885, 0.4050179211469534, 0.8028673835125448, 0.5591397849462365, 0.5501792114695341, 0.6523297491039427, 0.34587813620071683, 0.24551971326164876, 0.4157706093189964, 0.7114695340501792, 0.33154121863799285, 0.47491039426523296, 0.4050179211469534, 0.5913978494623656, 0.3673835125448029, 0.6093189964157706, 0.41397849462365593, 0.45340501792114696, 0.2670250896057348, 1.0, 0.510752688172043, 0.1863799283154122, 0.32437275985663083], [0.5829081632653061, 0.2423469387755102, 0.7385204081632653, 0.3405612244897959, 0.5114795918367347, 0.2461734693877551, 0.12244897959183673, 0.7908163265306123, 0.3903061224489796, 0.3762755102040816, 0.5012755102040817, 0.3086734693877551, 0.26913265306122447, 0.3635204081632653, 0.27168367346938777, 0.1913265306122449, 0.2653061224489796, 0.3163265306122449, 0.34311224489795916, 0.3252551020408163, 0.8775510204081632, 0.25637755102040816, 0.45663265306122447, 1.0, 0.1913265306122449, 0.8788265306122449, 0.6377551020408163], [0.771004942339374, 0.4464579901153213, 0.8632619439868204, 0.6902800658978583, 0.48929159802306427, 0.49093904448105435, 0.39373970345963755, 1.0, 0.48105436573311366, 0.6095551894563427, 0.8484349258649094, 0.5436573311367381, 0.514003294892916, 0.8401976935749588, 0.9522240527182867, 0.71499176276771, 0.5667215815485996, 0.3261943986820428, 0.8253706754530478, 0.729818780889621, 0.29983525535420097, 0.4135090609555189, 0.6688632619439868, 0.9373970345963756, 0.9390444810543658, 0.7116968698517299, 0.7084019769357496]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(tdata, tlabels, verbose=1, epochs=200, shuffle=True, callbacks=[TrainingGuardCallback()])"
      ],
      "metadata": {
        "id": "VA3LE1c57_ci",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "881da605-8d7c-4ba9-c828-adef8b06d6db"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "4/4 [==============================] - 0s 48ms/step - loss: 0.8475 - accuracy: 0.7347\n",
            "Epoch 2/200\n",
            "4/4 [==============================] - 0s 42ms/step - loss: 0.9621 - accuracy: 0.7143\n",
            "Epoch 3/200\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 1.0889 - accuracy: 0.6122\n",
            "Epoch 4/200\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1.1654 - accuracy: 0.6327\n",
            "Epoch 5/200\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 1.1546 - accuracy: 0.5918\n",
            "Epoch 6/200\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 1.0776 - accuracy: 0.6327\n",
            "Epoch 7/200\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 0.9136 - accuracy: 0.7143\n",
            "Epoch 8/200\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1.0717 - accuracy: 0.6939\n",
            "Epoch 9/200\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 0.9768 - accuracy: 0.6735\n",
            "Epoch 10/200\n",
            "4/4 [==============================] - 0s 38ms/step - loss: 1.1163 - accuracy: 0.6837\n",
            "Epoch 11/200\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 0.7727 - accuracy: 0.8061\n",
            "Epoch 12/200\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1.2140 - accuracy: 0.6224\n",
            "Epoch 13/200\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 0.8856 - accuracy: 0.6735\n",
            "Epoch 14/200\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 1.0767 - accuracy: 0.7041\n",
            "Epoch 15/200\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1.2068 - accuracy: 0.6429\n",
            "Epoch 16/200\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 0.6250 - accuracy: 0.7959\n",
            "Epoch 17/200\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 0.9943 - accuracy: 0.7041\n",
            "Epoch 18/200\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 0.9143 - accuracy: 0.6837\n",
            "Epoch 19/200\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1.0264 - accuracy: 0.6633\n",
            "Epoch 20/200\n",
            "4/4 [==============================] - 0s 38ms/step - loss: 0.9166 - accuracy: 0.7041\n",
            "Epoch 21/200\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 0.8077 - accuracy: 0.6837\n",
            "Epoch 22/200\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 0.7314 - accuracy: 0.7755\n",
            "Epoch 23/200\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 0.7256 - accuracy: 0.7857\n",
            "Epoch 24/200\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 0.9356 - accuracy: 0.6939\n",
            "Epoch 25/200\n",
            "4/4 [==============================] - 0s 38ms/step - loss: 0.8014 - accuracy: 0.7551\n",
            "Epoch 26/200\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 0.7853 - accuracy: 0.7449\n",
            "Epoch 27/200\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 0.9676 - accuracy: 0.7041\n",
            "Epoch 28/200\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 0.6899 - accuracy: 0.8061\n",
            "Epoch 29/200\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 0.6824 - accuracy: 0.7959\n",
            "Epoch 30/200\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 0.7231 - accuracy: 0.7959\n",
            "Epoch 31/200\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 0.7391 - accuracy: 0.7449\n",
            "Epoch 32/200\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 0.4959 - accuracy: 0.8469\n",
            "Epoch 33/200\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 0.6260 - accuracy: 0.7551\n",
            "Epoch 34/200\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 0.8054 - accuracy: 0.7551\n",
            "Epoch 35/200\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 0.9402 - accuracy: 0.7551\n",
            "Epoch 36/200\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 0.7400 - accuracy: 0.7755\n",
            "Epoch 37/200\n",
            "4/4 [==============================] - 0s 40ms/step - loss: 0.5367 - accuracy: 0.7959\n",
            "Epoch 38/200\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 0.7106 - accuracy: 0.7245\n",
            "Epoch 39/200\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1.0214 - accuracy: 0.6837\n",
            "Epoch 40/200\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 0.8231 - accuracy: 0.7959\n",
            "Epoch 41/200\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 0.7021 - accuracy: 0.7755\n",
            "Epoch 42/200\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 0.6999 - accuracy: 0.7755\n",
            "Epoch 43/200\n",
            "4/4 [==============================] - 0s 44ms/step - loss: 1.1645 - accuracy: 0.6224\n",
            "Epoch 44/200\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 0.7912 - accuracy: 0.7449\n",
            "Epoch 45/200\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 0.7699 - accuracy: 0.7755\n",
            "Epoch 46/200\n",
            "4/4 [==============================] - 0s 44ms/step - loss: 0.7630 - accuracy: 0.7653\n",
            "Epoch 47/200\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 0.8397 - accuracy: 0.7143\n",
            "Epoch 48/200\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 0.8832 - accuracy: 0.7347\n",
            "Epoch 49/200\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 0.8451 - accuracy: 0.7653\n",
            "Epoch 50/200\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 0.6406 - accuracy: 0.7755\n",
            "Epoch 51/200\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 1.0160 - accuracy: 0.6837\n",
            "Epoch 52/200\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 0.8514 - accuracy: 0.6939\n",
            "Epoch 53/200\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 0.8154 - accuracy: 0.7245\n",
            "Epoch 54/200\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 0.8724 - accuracy: 0.7551\n",
            "Epoch 55/200\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 1.1316 - accuracy: 0.6327\n",
            "Epoch 56/200\n",
            "4/4 [==============================] - 0s 38ms/step - loss: 0.9639 - accuracy: 0.6224\n",
            "Epoch 57/200\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 0.8293 - accuracy: 0.7143\n",
            "Epoch 58/200\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 0.6571 - accuracy: 0.7755\n",
            "Epoch 59/200\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 0.8678 - accuracy: 0.6939\n",
            "Epoch 60/200\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 0.7476 - accuracy: 0.7041\n",
            "Epoch 61/200\n",
            "4/4 [==============================] - 0s 39ms/step - loss: 0.6207 - accuracy: 0.8061\n",
            "Epoch 62/200\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 0.7643 - accuracy: 0.7755\n",
            "Epoch 63/200\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 0.7141 - accuracy: 0.8265\n",
            "Epoch 64/200\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 0.6181 - accuracy: 0.8469\n",
            "Epoch 65/200\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 0.7146 - accuracy: 0.7653\n",
            "Epoch 66/200\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 0.6844 - accuracy: 0.7755\n",
            "Epoch 67/200\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 0.7626 - accuracy: 0.7449\n",
            "Epoch 68/200\n",
            "4/4 [==============================] - 0s 45ms/step - loss: 0.4910 - accuracy: 0.8571\n",
            "Epoch 69/200\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 0.8361 - accuracy: 0.7143\n",
            "Epoch 70/200\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 0.6896 - accuracy: 0.7755\n",
            "Epoch 71/200\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 0.5495 - accuracy: 0.8061\n",
            "Epoch 72/200\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 0.5600 - accuracy: 0.8061\n",
            "Epoch 73/200\n",
            "4/4 [==============================] - 0s 39ms/step - loss: 0.5604 - accuracy: 0.8367\n",
            "Epoch 74/200\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 0.6114 - accuracy: 0.7755\n",
            "Epoch 75/200\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 0.5833 - accuracy: 0.8061\n",
            "Epoch 76/200\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 0.6639 - accuracy: 0.7551\n",
            "Epoch 77/200\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 0.5201 - accuracy: 0.8265\n",
            "Epoch 78/200\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 0.5955 - accuracy: 0.8367\n",
            "Epoch 79/200\n",
            "4/4 [==============================] - 0s 40ms/step - loss: 0.4074 - accuracy: 0.8469\n",
            "Epoch 80/200\n",
            "4/4 [==============================] - 0s 38ms/step - loss: 0.5624 - accuracy: 0.8367\n",
            "Epoch 81/200\n",
            "4/4 [==============================] - 0s 40ms/step - loss: 0.5891 - accuracy: 0.8265\n",
            "Epoch 82/200\n",
            "4/4 [==============================] - 0s 44ms/step - loss: 0.5712 - accuracy: 0.8061\n",
            "Epoch 83/200\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 0.5740 - accuracy: 0.8163\n",
            "Epoch 84/200\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 0.6140 - accuracy: 0.8265\n",
            "Epoch 85/200\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 0.7731 - accuracy: 0.7143\n",
            "Epoch 86/200\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 0.7112 - accuracy: 0.7857\n",
            "Epoch 87/200\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 0.6315 - accuracy: 0.7959\n",
            "Epoch 88/200\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 0.7946 - accuracy: 0.7551\n",
            "Epoch 89/200\n",
            "4/4 [==============================] - 0s 38ms/step - loss: 0.7282 - accuracy: 0.7857\n",
            "Epoch 90/200\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 0.8363 - accuracy: 0.7449\n",
            "Epoch 91/200\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 0.6517 - accuracy: 0.7551\n",
            "Epoch 92/200\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 0.6558 - accuracy: 0.8163\n",
            "Epoch 93/200\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 0.6688 - accuracy: 0.7755\n",
            "Epoch 94/200\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 0.7734 - accuracy: 0.7347\n",
            "Epoch 95/200\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 0.5359 - accuracy: 0.8265\n",
            "Epoch 96/200\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 0.5463 - accuracy: 0.8469\n",
            "Epoch 97/200\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 0.8281 - accuracy: 0.7347\n",
            "Epoch 98/200\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 0.6607 - accuracy: 0.8061\n",
            "Epoch 99/200\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 0.6521 - accuracy: 0.8265\n",
            "Epoch 100/200\n",
            "4/4 [==============================] - 0s 39ms/step - loss: 0.5791 - accuracy: 0.8673\n",
            "Epoch 101/200\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 0.6343 - accuracy: 0.7449\n",
            "Epoch 102/200\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.5127 - accuracy: 0.7812\n",
            "Training goals met. Training has been stopped!!\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 0.5033 - accuracy: 0.7857\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5952f4c390>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(testd, testl, verbose=0)\n",
        "print(\"Model loss (test data): {:.3f} \\t Model accuracy (test data): {:.2f}%\".format(loss, accuracy*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFvsEVQP8KsI",
        "outputId": "aa9b7cbe-e98a-4ca2-8dcb-6e440044ba01"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loss (test data): 1.937 \t Model accuracy (test data): 42.86%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def save_aug_sets(dataset, path, offset):\n",
        "  os.makedirs(path, exist_ok=True) # create dir if not exists\n",
        "\n",
        "  for key in dataset:\n",
        "    sampleid = offset\n",
        "\n",
        "    for set in dataset[key]:\n",
        "      if set.aug is not True:\n",
        "        continue\n",
        "\n",
        "      filepath = path + \"/#\" + str(set.id+1).zfill(2) + \"_\" + str(sampleid)  + \".txt\"\n",
        "      sampleid += 1\n",
        "      \n",
        "      with open(filepath, 'w') as f:\n",
        "\n",
        "        for k in set.keystrokes:\n",
        "          f.write(\"\\t{}, \\t{}, \\t{}\".format(k.key, k.down, k.up))\n",
        "          f.write('\\n')"
      ],
      "metadata": {
        "id": "gmNErS0-IM_7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_aug_sets(dataset, \"drive/MyDrive/Keystrokes/Augmented\", 3)"
      ],
      "metadata": {
        "id": "9soQFCsVKiHE"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}